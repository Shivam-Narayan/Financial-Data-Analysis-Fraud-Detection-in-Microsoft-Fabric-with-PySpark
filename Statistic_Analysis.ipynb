{"cells":[{"cell_type":"code","source":["#Hi, Welcome to the Project1 : Financial Transactions Dataset: Analytics\n","print(\"Financial Transactions Dataset: Analytics\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"a32129fa-5b8b-427f-a9b8-0d65d191327e","normalized_state":"finished","queued_time":"2024-11-10T18:27:09.4693556Z","session_start_time":null,"execution_start_time":"2024-11-10T18:27:14.08016Z","execution_finish_time":"2024-11-10T18:27:16.3119321Z","parent_msg_id":"dc2195d6-6234-4a10-b2da-bc2aca68d03a"},"text/plain":"StatementMeta(, a32129fa-5b8b-427f-a9b8-0d65d191327e, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Financial Transactions Dataset: Analytics\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cefeb6ec-ea4b-4a0b-82a1-b11112fcd092"},{"cell_type":"code","source":["#Making the Dataframes from CSV files in Lakehouse\n","transactionsdf  = spark.read.csv(\"abfss://Statistic_Analysis@onelake.dfs.fabric.microsoft.com/Statistic_Analysis.Lakehouse/Files/transactions_data.csv\",header = True,inferSchema = True)\n","cardsdf = spark.read.csv(\"abfss://Statistic_Analysis@onelake.dfs.fabric.microsoft.com/Statistic_Analysis.Lakehouse/Files/cards_data.csv\",header=True,inferSchema=True)\n","clientdf = spark.read.csv(\"abfss://Statistic_Analysis@onelake.dfs.fabric.microsoft.com/Statistic_Analysis.Lakehouse/Files/users_data.csv\",header=True,inferSchema=True)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"a32129fa-5b8b-427f-a9b8-0d65d191327e","normalized_state":"finished","queued_time":"2024-11-10T18:27:09.5891248Z","session_start_time":null,"execution_start_time":"2024-11-10T18:27:16.7322153Z","execution_finish_time":"2024-11-10T18:27:33.4358954Z","parent_msg_id":"9810cddc-310c-4a0f-8bc5-a1b76b874a52"},"text/plain":"StatementMeta(, a32129fa-5b8b-427f-a9b8-0d65d191327e, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"aec08ba3-eceb-4b4f-9e55-c8463b23b257"},{"cell_type":"code","source":["# Try reading the json file with the multiline to handle json object data\n","Mcodedf = spark.read.option(\"multiLine\", \"true\").json('abfss://Statistic_Analysis@onelake.dfs.fabric.microsoft.com/Statistic_Analysis.Lakehouse/Files/mcc_codes.json')\n","#display(Mcodedf) ,-- Mcodes are taken as Column names and Description as Rows\n","\n","# Approach - Extract Column(Mcodes) and Description(Row) in Dataframes\n","columns = Mcodedf.columns\n","first_row = Mcodedf.head()\n","descriptions = [first_row[col] for col in columns]\n","# Create a DataFrame with MCCcode and description\n","Mcodedf = spark.createDataFrame(zip(columns, descriptions), [\"MCCcode\", \"Description\"])\n","Mcodedf.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"a32129fa-5b8b-427f-a9b8-0d65d191327e","normalized_state":"finished","queued_time":"2024-11-10T18:27:09.7043241Z","session_start_time":null,"execution_start_time":"2024-11-10T18:27:33.954131Z","execution_finish_time":"2024-11-10T18:27:36.4510678Z","parent_msg_id":"f38bd0a4-fb76-43bd-9252-db266fb0b01f"},"text/plain":"StatementMeta(, a32129fa-5b8b-427f-a9b8-0d65d191327e, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-------+--------------------+\n|MCCcode|         Description|\n+-------+--------------------+\n|   1711|Heating, Plumbing...|\n|   3000|          Steelworks|\n|   3001|Steel Products Ma...|\n|   3005|Miscellaneous Met...|\n|   3006|Miscellaneous Fab...|\n|   3007|Coated and Lamina...|\n|   3008|Steel Drums and B...|\n|   3009|Fabricated Struct...|\n|   3058|Tools, Parts, Sup...|\n|   3066|Miscellaneous Metals|\n|   3075|Bolt, Nut, Screw,...|\n|   3132|       Leather Goods|\n|   3144|Floor Covering St...|\n|   3174|Upholstery and Dr...|\n|   3256|Brick, Stone, and...|\n|   3260|Pottery and Ceramics|\n|   3359|Non-Ferrous Metal...|\n|   3387|Electroplating, P...|\n|   3389|Non-Precious Meta...|\n|   3390|Miscellaneous Met...|\n+-------+--------------------+\nonly showing top 20 rows\n\n"]}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"ceccce77-63ff-4f2c-8de5-c04079f77973"},{"cell_type":"markdown","source":["### **Exploratory Data Analysis**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7bbc211b-2bb7-40b4-a3b0-af2a1989d60a"},{"cell_type":"markdown","source":["###### **TRANSACTIONS DATA** - transactiondf DataFrame"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"089385f3-d819-4459-b3af-665192ba82c2"},{"cell_type":"code","source":["# this step will print the Schema of the dataframe\n","transactionsdf.printSchema()\n","# this step can be used to check for the datatypes of columns of the dataframe\n","transactionsdf.dtypes\n","# Find the Number of Null values in the dataframe and replace with suitable replacements if necessary\n","from pyspark.sql.functions import col, sum \n","# Create a new DataFrame where each column indicates 1 for null and 0 for non-null\n","null_counts = transactionsdf.select(\n","    [col(c).isNull().cast(\"int\").alias(c) for c in transactionsdf.columns]\n",")\n","# Sum the null values in each column\n","transactionnullcount = null_counts.agg(\n","    *[sum(col(c)).alias(c) for c in transactionsdf.columns]\n",")\n","transactionnullcount.show()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"submitted","livy_statement_state":"running","session_id":"a32129fa-5b8b-427f-a9b8-0d65d191327e","normalized_state":"running","queued_time":"2024-11-10T18:27:09.8821603Z","session_start_time":null,"execution_start_time":"2024-11-10T18:27:36.8398976Z","execution_finish_time":null,"parent_msg_id":"d248b1d8-fe4e-4f37-a053-1341627f60d4"},"text/plain":"StatementMeta(, a32129fa-5b8b-427f-a9b8-0d65d191327e, 6, Submitted, Running, Running)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- id: integer (nullable = true)\n |-- date: timestamp (nullable = true)\n |-- client_id: integer (nullable = true)\n |-- card_id: integer (nullable = true)\n |-- amount: string (nullable = true)\n |-- use_chip: string (nullable = true)\n |-- merchant_id: integer (nullable = true)\n |-- merchant_city: string (nullable = true)\n |-- merchant_state: string (nullable = true)\n |-- zip: double (nullable = true)\n |-- mcc: integer (nullable = true)\n |-- errors: string (nullable = true)\n\n+---+----+---------+-------+------+--------+-----------+-------------+--------------+-------+---+--------+\n| id|date|client_id|card_id|amount|use_chip|merchant_id|merchant_city|merchant_state|    zip|mcc|  errors|\n+---+----+---------+-------+------+--------+-----------+-------------+--------------+-------+---+--------+\n|  0|   0|        0|      0|     0|       0|          0|            0|       1563700|1652706|  0|13094522|\n+---+----+---------+-------+------+--------+-----------+-------------+--------------+-------+---+--------+\n\n"]}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"834b3bf7-4e02-4b74-88f0-a9be267db9bb"},{"cell_type":"code","source":["# Generating the count of unique values for merchant_city and merchant_state\n","CityCount = transactionsdf.select(\"merchant_city\").distinct().count()\n","StateCount = transactionsdf.select(\"merchant_state\").distinct().count()\n","print(f\"Different Cities: {CityCount} || Different States: {StateCount}\")\n","\n","#Adding New Column having both City-State in transactionsDF \n","from pyspark.sql.functions import col, concat_ws\n","transactionsdf = transactionsdf.withColumn(\"CityState\", concat_ws(\", \", col(\"merchant_city\"), col(\"merchant_state\")))\n","transactionsdf.select(\"merchant_city\", \"merchant_state\", \"CityState\").show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:10.1157249Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"974a953b-aaf4-4070-9c6a-c07cb9a08a23"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Different Cities: 12492 || Different States: 200\n+---------------+--------------+-------------------+\n|  merchant_city|merchant_state|          CityState|\n+---------------+--------------+-------------------+\n|         Beulah|            ND|         Beulah, ND|\n|     Bettendorf|            IA|     Bettendorf, IA|\n|          Vista|            CA|          Vista, CA|\n|    Crown Point|            IN|    Crown Point, IN|\n|        Harwood|            MD|        Harwood, MD|\n|          Bronx|            NY|          Bronx, NY|\n|         Beulah|            ND|         Beulah, ND|\n|         ONLINE|          NULL|             ONLINE|\n|         ONLINE|          NULL|             ONLINE|\n|       Flushing|            NY|       Flushing, NY|\n|       Pearland|            TX|       Pearland, TX|\n|       Brooklyn|            NY|       Brooklyn, NY|\n|         Beulah|            ND|         Beulah, ND|\n|        Kahului|            HI|        Kahului, HI|\n|North Hollywood|            CA|North Hollywood, CA|\n|     San Benito|            TX|     San Benito, TX|\n|           Erie|            PA|           Erie, PA|\n|          Vista|            CA|          Vista, CA|\n|         ONLINE|          NULL|             ONLINE|\n|North Hollywood|            CA|North Hollywood, CA|\n+---------------+--------------+-------------------+\nonly showing top 20 rows\n\n"]}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fbd274b7-1f39-4085-a35e-8c71b3312438"},{"cell_type":"markdown","source":["1. Handling Missing Values : Null Values/Error in values around the columns of dataframes\n","2. Ensuring the Datatypes : Convert the column into authentic datatypes like from string (if it contains symbols) to numeric, and dates in a proper timestamp format."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"180f745c-4419-491a-b2dc-4745d37d79c5"},{"cell_type":"code","source":["from pyspark.sql.functions import col, regexp_replace,to_date\n","from pyspark.sql.types import DoubleType\n","\n","#Transactiondf have amount column which contains Dollar sign $, cleaning it and converting to double Type by creating a function\n","def CleanAmount(amount):\n","    # Remove dollar signs, commas and keep the negative sign intact\n","    cleaned_amount = regexp_replace(amount, r\"[\\$,]\", \"\")  # Remove $, ,\n","    return cleaned_amount.cast(\"double\")\n","# Apply the function to the 'amount' column and create a new cleaned 'amount' column\n","transactionsdf = transactionsdf.withColumn(\"amount\", CleanAmount(col(\"amount\")))\n","transactionsdf = transactionsdf.withColumn(\"date\", to_date(col(\"date\"), 'yyyy-MM-dd HH:mm:ss'))\n","transactionsdf.show(5)  #transactionsdf.dtypes this will check the datatypes of the columns in TransactionDf"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:10.3895425Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"ad03660d-7374-41e9-a8ab-41b593eb09df"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-------+----------+---------+-------+------+-----------------+-----------+-------------+--------------+-------+----+------+---------------+\n|     id|      date|client_id|card_id|amount|         use_chip|merchant_id|merchant_city|merchant_state|    zip| mcc|errors|      CityState|\n+-------+----------+---------+-------+------+-----------------+-----------+-------------+--------------+-------+----+------+---------------+\n|7475327|2010-01-01|     1556|   2972| -77.0|Swipe Transaction|      59935|       Beulah|            ND|58523.0|5499|  NULL|     Beulah, ND|\n|7475328|2010-01-01|      561|   4575| 14.57|Swipe Transaction|      67570|   Bettendorf|            IA|52722.0|5311|  NULL| Bettendorf, IA|\n|7475329|2010-01-01|     1129|    102|  80.0|Swipe Transaction|      27092|        Vista|            CA|92084.0|4829|  NULL|      Vista, CA|\n|7475331|2010-01-01|      430|   2860| 200.0|Swipe Transaction|      27092|  Crown Point|            IN|46307.0|4829|  NULL|Crown Point, IN|\n|7475332|2010-01-01|      848|   3915| 46.41|Swipe Transaction|      13051|      Harwood|            MD|20776.0|5813|  NULL|    Harwood, MD|\n+-------+----------+---------+-------+------+-----------------+-----------+-------------+--------------+-------+----+------+---------------+\nonly showing top 5 rows\n\n"]}],"execution_count":6,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8911ca0e-e669-4039-99da-a5a74b426c28"},{"cell_type":"markdown","source":["###### **CARDS DATA** - cardsdf DataFrame"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd31b8f8-de4d-4825-b29f-cdf79e37424a"},{"cell_type":"code","source":["# this step will print the Schema of the dataframe\n","cardsdf.printSchema()\n","# this step can be used to check for the datatypes of columns of the dataframe\n","cardsdf.dtypes\n","# Find the Number of Null values in the dataframe and replace with suitable replacements if necessary\n","from pyspark.sql.functions import col, sum \n","# Create a new DataFrame where each column indicates 1 for null and 0 for non-null\n","null_counts = cardsdf.select(\n","    [col(c).isNull().cast(\"int\").alias(c) for c in cardsdf.columns]\n",")\n","# Sum the null values in each column\n","cardnullcount = null_counts.agg(\n","    *[sum(col(c)).alias(c) for c in cardsdf.columns]\n",")\n","cardnullcount.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:10.677869Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"286a1b0b-8fdc-41c2-8cc3-9dccec4b299c"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- id: integer (nullable = true)\n |-- client_id: integer (nullable = true)\n |-- card_brand: string (nullable = true)\n |-- card_type: string (nullable = true)\n |-- card_number: long (nullable = true)\n |-- expires: string (nullable = true)\n |-- cvv: integer (nullable = true)\n |-- has_chip: string (nullable = true)\n |-- num_cards_issued: integer (nullable = true)\n |-- credit_limit: string (nullable = true)\n |-- acct_open_date: string (nullable = true)\n |-- year_pin_last_changed: integer (nullable = true)\n |-- card_on_dark_web: string (nullable = true)\n\n+---+---------+----------+---------+-----------+-------+---+--------+----------------+------------+--------------+---------------------+----------------+\n| id|client_id|card_brand|card_type|card_number|expires|cvv|has_chip|num_cards_issued|credit_limit|acct_open_date|year_pin_last_changed|card_on_dark_web|\n+---+---------+----------+---------+-----------+-------+---+--------+----------------+------------+--------------+---------------------+----------------+\n|  0|        0|         0|        0|          0|      0|  0|       0|               0|           0|             0|                    0|               0|\n+---+---------+----------+---------+-----------+-------+---+--------+----------------+------------+--------------+---------------------+----------------+\n\n"]}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1ff67e00-a3b7-47d8-b172-12958109874d"},{"cell_type":"code","source":["#Converting the Credit Limit of the Card to Double using the function made above in transactiondf\n","cardsdf = cardsdf.withColumn(\"credit_limit\", CleanAmount(col(\"credit_limit\")))\n","cardsdf.dtypes"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:10.8970224Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"f9f0cc66-2289-4c2e-b778-3ac0310fee82"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"[('id', 'int'),\n ('client_id', 'int'),\n ('card_brand', 'string'),\n ('card_type', 'string'),\n ('card_number', 'bigint'),\n ('expires', 'string'),\n ('cvv', 'int'),\n ('has_chip', 'string'),\n ('num_cards_issued', 'int'),\n ('credit_limit', 'double'),\n ('acct_open_date', 'string'),\n ('year_pin_last_changed', 'int'),\n ('card_on_dark_web', 'string')]"},"metadata":{}}],"execution_count":8,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1f031da8-f006-43c1-89f1-5cb6a5c830ed"},{"cell_type":"code","source":["# Generating the count of unique values for card_brand and card_type\n","BrandCount = cardsdf.select(\"card_brand\").distinct().count()\n","TypeCount = cardsdf.select(\"card_type\").distinct().count()\n","print(f\"Different Brands: {BrandCount} || Different Types: {TypeCount}\")\n","# Check if card_on_dark_web column is skewed\n","cardsdf.groupBy(\"card_on_dark_web\").count().withColumnRenamed(\"card_on_dark_web\",\"Skewed\").show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:11.0865784Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"5c3a62ab-e5d1-42f5-8592-f573e4efdfb1"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Different Brands: 4 || Different Types: 3\n+------+-----+\n|Skewed|count|\n+------+-----+\n|    No| 6146|\n+------+-----+\n\n"]}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"959ba07a-cf68-405c-ae21-fed974ebe012"},{"cell_type":"markdown","source":["###### **CLIENT DATA** - clientdf DataFrame"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"65e60b87-b932-475a-89b0-d0d9418e88be"},{"cell_type":"code","source":["# this step will print the Schema of the dataframe\n","clientdf.printSchema()\n","# this step can be used to check for the datatypes of columns of the dataframe\n","clientdf.dtypes\n","# Find the Number of Null values in the dataframe and replace with suitable replacements if necessary\n","from pyspark.sql.functions import col, sum \n","# Create a new DataFrame where each column indicates 1 for null and 0 for non-null\n","null_counts = clientdf.select(\n","    [col(c).isNull().cast(\"int\").alias(c) for c in clientdf.columns]\n",")\n","# Sum the null values in each column\n","clientnullcount = null_counts.agg(\n","    *[sum(col(c)).alias(c) for c in clientdf.columns]\n",")\n","clientnullcount.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:11.2907379Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"1481796a-1173-4d6c-b876-a96a49cd21dd"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- id: integer (nullable = true)\n |-- current_age: integer (nullable = true)\n |-- retirement_age: integer (nullable = true)\n |-- birth_year: integer (nullable = true)\n |-- birth_month: integer (nullable = true)\n |-- gender: string (nullable = true)\n |-- address: string (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- per_capita_income: string (nullable = true)\n |-- yearly_income: string (nullable = true)\n |-- total_debt: string (nullable = true)\n |-- credit_score: integer (nullable = true)\n |-- num_credit_cards: integer (nullable = true)\n\n+---+-----------+--------------+----------+-----------+------+-------+--------+---------+-----------------+-------------+----------+------------+----------------+\n| id|current_age|retirement_age|birth_year|birth_month|gender|address|latitude|longitude|per_capita_income|yearly_income|total_debt|credit_score|num_credit_cards|\n+---+-----------+--------------+----------+-----------+------+-------+--------+---------+-----------------+-------------+----------+------------+----------------+\n|  0|          0|             0|         0|          0|     0|      0|       0|        0|                0|            0|         0|           0|               0|\n+---+-----------+--------------+----------+-----------+------+-------+--------+---------+-----------------+-------------+----------+------------+----------------+\n\n"]}],"execution_count":10,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7edd58a2-dc36-4627-a158-916e8a59bd13"},{"cell_type":"code","source":["#Converting datatypes of Income Columns : {per_capita_income,yearly_income} using the CleanAmount function \n","clientdf = clientdf.withColumn(\"per_capita_income\", CleanAmount(col(\"per_capita_income\")))\n","clientdf = clientdf.withColumn(\"yearly_income\", CleanAmount(col(\"yearly_income\")))\n","clientdf = clientdf.withColumn(\"total_debt\", CleanAmount(col(\"total_debt\")))\n","clientdf.dtypes"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:11.5778699Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"77661b81-9672-4143-8242-7b1b83ffd5a6"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"[('id', 'int'),\n ('current_age', 'int'),\n ('retirement_age', 'int'),\n ('birth_year', 'int'),\n ('birth_month', 'int'),\n ('gender', 'string'),\n ('address', 'string'),\n ('latitude', 'double'),\n ('longitude', 'double'),\n ('per_capita_income', 'double'),\n ('yearly_income', 'double'),\n ('total_debt', 'double'),\n ('credit_score', 'int'),\n ('num_credit_cards', 'int')]"},"metadata":{}}],"execution_count":11,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"18631c4f-9a26-49c7-972b-63e6d3b823a4"},{"cell_type":"code","source":["# Exploring the Distribution of Key Metrics: current_age, retirement_age, gender, income, credit_score\n","clientdf.select(\"current_age\").describe().show()\n","clientdf.select(\"retirement_age\").describe().show()\n","clientdf.groupBy(\"gender\").count().show()\n","clientdf.select(\"per_capita_income\").describe().show()\n","clientdf.select(\"yearly_income\").describe().show()\n","clientdf.select(\"credit_score\").describe().show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:11.7592505Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"2260e8c2-44ed-4c1e-b877-53fbab2e8703"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-------+------------------+\n|summary|       current_age|\n+-------+------------------+\n|  count|              2000|\n|   mean|           45.3915|\n| stddev|18.414091537014993|\n|    min|                18|\n|    max|               101|\n+-------+------------------+\n\n+-------+-----------------+\n|summary|   retirement_age|\n+-------+-----------------+\n|  count|             2000|\n|   mean|          66.2375|\n| stddev|3.628867328663937|\n|    min|               50|\n|    max|               79|\n+-------+-----------------+\n\n+------+-----+\n|gender|count|\n+------+-----+\n|Female| 1016|\n|  Male|  984|\n+------+-----+\n\n+-------+-----------------+\n|summary|per_capita_income|\n+-------+-----------------+\n|  count|             2000|\n|   mean|        23141.928|\n| stddev|11324.13735766499|\n|    min|              0.0|\n|    max|         163145.0|\n+-------+-----------------+\n\n+-------+-----------------+\n|summary|    yearly_income|\n+-------+-----------------+\n|  count|             2000|\n|   mean|        45715.882|\n| stddev|22992.61545631198|\n|    min|              1.0|\n|    max|         307018.0|\n+-------+-----------------+\n\n+-------+-----------------+\n|summary|     credit_score|\n+-------+-----------------+\n|  count|             2000|\n|   mean|         709.7345|\n| stddev|67.22194883334225|\n|    min|              480|\n|    max|              850|\n+-------+-----------------+\n\n"]}],"execution_count":12,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"77d2d97e-c1c8-4d24-ad94-a8eaf1cbf541"},{"cell_type":"code","source":["# Check if MCCcode and Description are valid\n","Mcodedf.printSchema()\n","\n","# Count of unique MCC codes\n","Mcodedf.select(\"MCCcode\").distinct().count()\n","\n","# Distribution of MCC codes\n","Mcodedf.groupBy(\"Description\").count().orderBy(\"count\", ascending=False).show()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:11.9734822Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"45d293b1-4c4c-47be-90da-cbe06ba58da5"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- MCCcode: string (nullable = true)\n |-- Description: string (nullable = true)\n\n+--------------------+-----+\n|         Description|count|\n+--------------------+-----+\n|  Passenger Railways|    2|\n|Floor Covering St...|    1|\n|Heating, Plumbing...|    1|\n|Steel Drums and B...|    1|\n|Tools, Parts, Sup...|    1|\n|Miscellaneous Metals|    1|\n|       Leather Goods|    1|\n|Miscellaneous Fab...|    1|\n|Steel Products Ma...|    1|\n|Miscellaneous Met...|    1|\n|Fabricated Struct...|    1|\n|Coated and Lamina...|    1|\n|          Steelworks|    1|\n|Bolt, Nut, Screw,...|    1|\n|Industrial Equipm...|    1|\n|            Ironwork|    1|\n|Non-Ferrous Metal...|    1|\n|Non-Precious Meta...|    1|\n|Heat Treating Met...|    1|\n|Miscellaneous Met...|    1|\n+--------------------+-----+\nonly showing top 20 rows\n\n"]}],"execution_count":13,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"10381fcf-c580-4535-ae74-e336e2604441"},{"cell_type":"markdown","source":["###### **Writing DataFrames to Delta Tables in Lakehouse**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"29f1c93a-4765-46a1-a851-19e6d173f776"},{"cell_type":"markdown","source":["Saving the DataFrames (transactionsdf, cardsdf, clientdf, mcodedf) as Delta tables in the Lakehouse. The primary goal is to make the data easily accessible for analysis using SQL. By writing the DataFrames as Delta tables, we can leverage SQL queries for further exploration, aggregations, and advanced analytics within the Lakehouse environment. This enables us to use a familiar and efficient method of interacting with the data, optimizing it for both batch and real-time analytics."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"38226ce2-8d08-4547-be1c-9997220f5c23"},{"cell_type":"code","source":["'''\n","# Define the base path where the tables will be stored\n","base_path = \"abfss://Statistic_Analysis@onelake.dfs.fabric.microsoft.com/Statistic_Analysis.Lakehouse/Tables/\"\n","\n","# Write transactionsdf as a Delta table\n","transactionsdf.write.format(\"delta\").mode(\"overwrite\").save(base_path + \"transactions_table\")\n","\n","# Write cardsdf as a Delta table\n","cardsdf.write.format(\"delta\").mode(\"overwrite\").save(base_path + \"cards_table\")\n","\n","# Write clientdf as a Delta table\n","clientdf.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(base_path + \"client_table\")\n","\n","# Write mcodedf as a Delta table\n","Mcodedf.write.format(\"delta\").mode(\"overwrite\").save(base_path + \"mcc_codes_table\")\n","'''\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:12.5318229Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"bacde047-5abb-42d3-aedb-ff7559192df1"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"execute_result","execution_count":47,"data":{"text/plain":"'\\n# Define the base path where the tables will be stored\\nbase_path = \"abfss://Statistic_Analysis@onelake.dfs.fabric.microsoft.com/Statistic_Analysis.Lakehouse/Tables/\"\\n\\n# Write transactionsdf as a Delta table\\ntransactionsdf.write.format(\"delta\").mode(\"overwrite\").save(base_path + \"transactions_table\")\\n\\n# Write cardsdf as a Delta table\\ncardsdf.write.format(\"delta\").mode(\"overwrite\").save(base_path + \"cards_table\")\\n\\n# Write clientdf as a Delta table\\nclientdf.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(base_path + \"client_table\")\\n\\n# Write mcodedf as a Delta table\\nMcodedf.write.format(\"delta\").mode(\"overwrite\").save(base_path + \"mcc_codes_table\")\\n'"},"metadata":{}}],"execution_count":14,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5900e45a-6630-4a61-b598-42a4b5189f26"},{"cell_type":"markdown","source":[" ### **Transaction Data Analysis**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ad3e3155-ad8d-47a5-ad02-0c7c1d4c8c37"},{"cell_type":"markdown","source":["###### **Year-over-Year Transaction Trends**: This analysis tracks the year-over-year (YOY) change in transaction counts, helping to identify significant increases or decreases in transaction volumes between consecutive years. By calculating the percentage change and comparing current year data with the previous year using the lag function, businesses can spot trends and plan strategies accordingly. The analysis highlights the largest increase and decrease in transaction counts, providing insights for future planning and identifying periods of growth or decline."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"72e9c909-bfa8-470a-85f2-d1e987396150"},{"cell_type":"code","source":["from pyspark.sql.functions import year, lag, col\n","from pyspark.sql.window import Window\n","\n","transactions_per_year = transactionsdf.groupBy(year(\"date\").alias(\"year\")).count().orderBy(\"year\")\n","# Calculate percentage change in TransactionCounts YOY(YearOverYear)\n","window_spec = Window.orderBy(\"year\")\n","\n","#Adding Column for PreviousYearTransactionCount using Lag function\n","transactions_per_year = transactions_per_year.withColumn(\n","    \"previous_year_count\", lag(\"count\", 1).over(window_spec)\n",")\n","\n","#Adding Column indicating Percentage Difference of TransactionCount of CurrentYear and PreviousYear\n","transactions_per_year = transactions_per_year.withColumn(\n","    \"percentage_change\", ((col(\"count\") - col(\"previous_year_count\")) / col(\"previous_year_count\")) * 100\n",")\n","transactions_per_year.show()\n","\n","transactions_per_year_filtered = transactions_per_year.filter(col(\"previous_year_count\").isNotNull())\n","\n","# Finding Trends of TransactionCounts over Years\n","max_increase = transactions_per_year_filtered.orderBy(col(\"percentage_change\").desc()).first()\n","max_decrease = transactions_per_year_filtered.orderBy(col(\"percentage_change\").asc()).first()\n","\n","# Print the Insights from above Data\n","print(f\"Largest positive increase in transactions: Year {max_increase['year']} with {max_increase['percentage_change']}% increase\")\n","print(f\"Largest negative decrease in transactions: Year {max_decrease['year']} with {max_decrease['percentage_change']}% decrease\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:12.7787373Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"f0d7ce0d-15fd-4d88-b906-d3efc4ee31a9"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+----+-------+-------------------+--------------------+\n|year|  count|previous_year_count|   percentage_change|\n+----+-------+-------------------+--------------------+\n|2010|1240880|               NULL|                NULL|\n|2011|1290770|            1240880|    4.02053381471214|\n|2012|1321672|            1290770|   2.394074854544187|\n|2013|1352808|            1321672|  2.3558038605644973|\n|2014|1365537|            1352808|  0.9409317508471269|\n|2015|1388065|            1365537|  1.6497539063386784|\n|2016|1392117|            1388065| 0.29191716526243366|\n|2017|1399308|            1392117|  0.5165514105495443|\n|2018|1394792|            1399308|-0.32273094986950696|\n|2019|1159966|            1394792|  -16.83591531927341|\n+----+-------+-------------------+--------------------+\n\nLargest positive increase in transactions: Year 2011 with 4.02053381471214% increase\nLargest negative decrease in transactions: Year 2019 with -16.83591531927341% decrease\n"]}],"execution_count":15,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"28d7c781-44e8-426a-933f-c4291be98c81"},{"cell_type":"markdown","source":["###### **Transaction Trends by Month and Weekday**: This analysis identifies transaction patterns by grouping data into monthly and weekday segments, providing valuable insights into transaction volumes across different time periods. The use of `date_format` function helps in creating `MonthYear` and `weekday_name` columns, allowing us to group and count transactions efficiently. By identifying the months and weekdays with the highest and lowest transaction counts, businesses can optimize strategies for peak times and address low-traffic periods."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a6594209-26a3-43a6-9d01-9f7828850746"},{"cell_type":"code","source":["from pyspark.sql.functions import dayofweek, date_format, col, when\n","#use of date_format function and \n","# Transaction count per month using MonthYear Column\n","transactions_per_month = transactionsdf.withColumn(\"MonthYear\", date_format(\"date\", \"MMMyyyy\")\n",").groupBy(\"MonthYear\").count().withColumnRenamed(\"count\", \"TotalTransactions\").orderBy(col(\"TotalTransactions\").desc())\n","\n","# Transaction count per Day using Weekdays as legends\n","transactions_per_weekday = transactionsdf.withColumn(\"weekday_name\", date_format(\"date\", \"EEEE\")\n",").groupBy(\"weekday_name\").count().withColumnRenamed(\"count\", \"TotalTransactions\").orderBy(col(\"TotalTransactions\").desc())\n","\n","'''\n","transactions_per_month.show()\n","transactions_per_weekday.show()\n","'''\n","# Print MaxCount and MinCoutn of transactions per MonthYear\n","max_month = transactions_per_month.orderBy(col(\"TotalTransactions\").desc()).first()\n","min_month = transactions_per_month.orderBy(col(\"TotalTransactions\").asc()).first()\n","print(\"Transaction Trends over MonthYear:\")\n","print(f\"MonthYEar with Maximum Transactions: {max_month['MonthYear']} with {max_month['TotalTransactions']} transactions\")\n","print(f\"MonthYear with Minimum Transactions: {min_month['MonthYear']} with {min_month['TotalTransactions']} transactions\")\n","\n","# Print MaxCount and MinCoutn of transactions per weekday\n","max_weekday = transactions_per_weekday.orderBy(col(\"TotalTransactions\").desc()).first()\n","min_weekday = transactions_per_weekday.orderBy(col(\"TotalTransactions\").asc()).first()\n","print(\"\\nTransaction Trends over WeekDays:\")\n","print(f\"Weekday with Maximum Transactions: {max_weekday['weekday_name']} with {max_weekday['TotalTransactions']} transactions\")\n","print(f\"Weekday with Minimum Transactions: {min_weekday['weekday_name']} with {min_weekday['TotalTransactions']} transactions\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:12.9450692Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"47e6c017-7e08-43fd-a317-d3b602b869ce"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Transaction Trends over MonthYear:\nMonthYEar with Maximum Transactions: Dec2015 with 119396 transactions\nMonthYear with Minimum Transactions: Feb2010 with 93470 transactions\n\nTransaction Trends over WeekDays:\nWeekday with Maximum Transactions: Thursday with 1918666 transactions\nWeekday with Minimum Transactions: Friday with 1895372 transactions\n"]}],"execution_count":16,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4502de61-4a38-4aa3-b0ff-fbfca4ac95e6"},{"cell_type":"code","source":["transactionsdf.describe(\"amount\").show()\n","# TransactionCounts for Debit and Credit Cards\n","transactionsdf.withColumn(\"Amount_Type\", when(col(\"amount\") > 0, \"Credit\").otherwise(\"Debit\")\n",").groupBy(\"Amount_Type\").count().withColumnRenamed('count','TransactionCount').show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:13.128606Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"6457f049-5279-4cf0-89ab-96bd666ad77f"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-------+-----------------+\n|summary|           amount|\n+-------+-----------------+\n|  count|         13305915|\n|   mean|42.97603902324677|\n| stddev|81.65574765375865|\n|    min|           -500.0|\n|    max|           6820.2|\n+-------+-----------------+\n\n+-----------+----------------+\n|Amount_Type|TransactionCount|\n+-----------+----------------+\n|     Credit|        12635227|\n|      Debit|          670688|\n+-----------+----------------+\n\n"]}],"execution_count":17,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"de9eacba-97a6-4c7e-af33-d4116c9c28cd"},{"cell_type":"code","source":["# We can take Certain other factors in it as well Like the State and City in this\n","transactionsdf.withColumn(\n","    \"Amount_Type\", when(col(\"amount\") > 0, \"Credit\").otherwise(\"Debit\")\n",").withColumn(\n","    \"CityState\", concat_ws(\", \", col(\"merchant_city\"), col(\"merchant_state\"))\n",").groupBy(\n","    \"Amount_Type\", \"CityState\", \"zip\"\n",").count().withColumnRenamed(\"count\", \"TransactionCount\").show()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:13.3997812Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"262f06e4-a58e-4f33-89a6-0df15441c3e9"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-----------+-----------------+-------+----------------+\n|Amount_Type|        CityState|    zip|TransactionCount|\n+-----------+-----------------+-------+----------------+\n|     Credit|      Concord, NC|28025.0|            6023|\n|     Credit|     Glendale, CA|91205.0|            6065|\n|     Credit|      Wooster, OH|44691.0|            8240|\n|     Credit|        Cocoa, FL|32926.0|            4856|\n|      Debit|    Henderson, NV|89002.0|             480|\n|     Credit|         Novi, MI|48377.0|            3925|\n|     Credit|     Fairdale, KY|40118.0|             152|\n|     Credit|     Franklin, TX|77856.0|            4022|\n|     Credit|Green Village, NJ| 7935.0|             782|\n|     Credit|  Chino Hills, CA|91709.0|            1722|\n|     Credit|      Bristol, CT| 6010.0|            1218|\n|      Debit|    Brunswick, OH|44212.0|             729|\n|     Credit|      Oshkosh, WI|54902.0|             871|\n|     Credit|         Avon, CT| 6001.0|             344|\n|     Credit|  Marcus Hook, PA|19061.0|             668|\n|     Credit|  Church Hill, TN|37642.0|             673|\n|     Credit|       Avenel, NJ| 7001.0|             292|\n|     Credit|    Frankfort, IN|46041.0|            5774|\n|      Debit|Camano Island, WA|98282.0|             425|\n|     Credit|    Fort Knox, KY|40121.0|            3110|\n+-----------+-----------------+-------+----------------+\nonly showing top 20 rows\n\n"]}],"execution_count":18,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f514dc10-c574-4cc1-a036-a55700f96090"},{"cell_type":"markdown","source":["###### **Online Transactions Analysis**: This analysis identifies and tracks online transactions by categorizing them into Credit and Debit, filtering for transactions where the city is \"ONLINE\" and the zip code is missing. By grouping the data by CityState and Year, it reveals trends and patterns in online transactions over time. This helps businesses monitor online activity and adjust strategies based on transaction volumes by region and year."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d192457d-b846-468e-9d07-7474eec1486e"},{"cell_type":"code","source":["OnlineTransactionsdf = transactionsdf.withColumn(\"Amount_Type\", when(col(\"amount\") > 0, \"Credit\").otherwise(\"Debit\")\n",").filter((col(\"merchant_city\") == \"ONLINE\") & col(\"zip\").isNull())\n","OnlineTransactionsdf.show()\n","OnlineTransactionsdf = OnlineTransactionsdf.withColumn(\"Year\",date_format(\"date\",\"yyyy\"))\n","OnlineTransactionsdf.groupBy(\"CityState\",\"Year\").count().withColumnRenamed('count','TotalTransactions').orderBy(col(\"Year\").desc()).show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:13.6043341Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"c84b20f9-6960-410c-8439-ff39130abd53"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-------+----------+---------+-------+------+------------------+-----------+-------------+--------------+----+----+------+---------+-----------+\n|     id|      date|client_id|card_id|amount|          use_chip|merchant_id|merchant_city|merchant_state| zip| mcc|errors|CityState|Amount_Type|\n+-------+----------+---------+-------+------+------------------+-----------+-------------+--------------+----+----+------+---------+-----------+\n|7475335|2010-01-01|     1684|   2140| 26.46|Online Transaction|      39021|       ONLINE|          NULL|NULL|4784|  NULL|   ONLINE|     Credit|\n|7475336|2010-01-01|      335|   5131|261.58|Online Transaction|      50292|       ONLINE|          NULL|NULL|7801|  NULL|   ONLINE|     Credit|\n|7475346|2010-01-01|      394|   4717| 26.04|Online Transaction|      39021|       ONLINE|          NULL|NULL|4784|  NULL|   ONLINE|     Credit|\n|7475353|2010-01-01|      301|   3742| 10.17|Online Transaction|      39021|       ONLINE|          NULL|NULL|4784|  NULL|   ONLINE|     Credit|\n|7475356|2010-01-01|      566|   3439| 16.86|Online Transaction|      16798|       ONLINE|          NULL|NULL|4121|  NULL|   ONLINE|     Credit|\n|7475359|2010-01-01|     1127|   3869| 22.57|Online Transaction|      39021|       ONLINE|          NULL|NULL|4784|  NULL|   ONLINE|     Credit|\n|7475365|2010-01-01|      820|    127|270.22|Online Transaction|      73186|       ONLINE|          NULL|NULL|4814|  NULL|   ONLINE|     Credit|\n|7475367|2010-01-01|     1758|   4686| 87.09|Online Transaction|      17976|       ONLINE|          NULL|NULL|4900|  NULL|   ONLINE|     Credit|\n|7475372|2010-01-01|      566|   5577| 14.66|Online Transaction|      16798|       ONLINE|          NULL|NULL|4121|  NULL|   ONLINE|     Credit|\n|7475374|2010-01-01|     1684|     94| 12.67|Online Transaction|      39021|       ONLINE|          NULL|NULL|4784|  NULL|   ONLINE|     Credit|\n|7475391|2010-01-01|     1087|   1029| 62.98|Online Transaction|      17976|       ONLINE|          NULL|NULL|4900|  NULL|   ONLINE|     Credit|\n|7475394|2010-01-01|     1575|    224| 34.34|Online Transaction|      16798|       ONLINE|          NULL|NULL|4121|  NULL|   ONLINE|     Credit|\n|7475401|2010-01-01|     1449|    241|119.23|Online Transaction|      87530|       ONLINE|          NULL|NULL|4900|  NULL|   ONLINE|     Credit|\n|7475403|2010-01-01|      760|   5876| 52.98|Online Transaction|      39021|       ONLINE|          NULL|NULL|4784|  NULL|   ONLINE|     Credit|\n|7475425|2010-01-01|      241|     58| 38.11|Online Transaction|      17976|       ONLINE|          NULL|NULL|4900|  NULL|   ONLINE|     Credit|\n|7475427|2010-01-01|     1776|   4938| 66.05|Online Transaction|      39021|       ONLINE|          NULL|NULL|4784|  NULL|   ONLINE|     Credit|\n|7475432|2010-01-01|     1214|   5508| 51.58|Online Transaction|      98436|       ONLINE|          NULL|NULL|5192|  NULL|   ONLINE|     Credit|\n|7475463|2010-01-01|     1921|   1181| 40.89|Online Transaction|      59350|       ONLINE|          NULL|NULL|7922|  NULL|   ONLINE|     Credit|\n|7475468|2010-01-01|      121|   5952| 58.47|Online Transaction|      80770|       ONLINE|          NULL|NULL|7922|  NULL|   ONLINE|     Credit|\n|7475485|2010-01-01|      185|   5361| 15.89|Online Transaction|      39261|       ONLINE|          NULL|NULL|5815|  NULL|   ONLINE|     Credit|\n+-------+----------+---------+-------+------+------------------+-----------+-------------+--------------+----+----+------+---------+-----------+\nonly showing top 20 rows\n\n+---------+----+-----------------+\n|CityState|Year|TotalTransactions|\n+---------+----+-----------------+\n|   ONLINE|2019|           141071|\n|   ONLINE|2018|           169703|\n|   ONLINE|2017|           169999|\n|   ONLINE|2016|           171437|\n|   ONLINE|2015|           171202|\n|   ONLINE|2014|           161459|\n|   ONLINE|2013|           157141|\n|   ONLINE|2012|           147352|\n|   ONLINE|2011|           139514|\n|   ONLINE|2010|           134822|\n+---------+----+-----------------+\n\n"]}],"execution_count":19,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0881da90-ebf1-4061-b1c3-78d70cbb498d"},{"cell_type":"markdown","source":["### **Fraud Detection & Error Analysis** "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"636e5273-932e-4350-a271-97fe4a354429"},{"cell_type":"markdown","source":["###### **Fraudulent Transaction Analysis**: This analysis identifies merchants with fraudulent transactions by counting the instances and summing the amounts. It helps pinpoint merchants with higher fraud activity, enabling targeted risk management. The insights can be used to prioritize interventions and improve fraud detection systems."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ea305768-a202-4ef1-963a-446e8fc97002"},{"cell_type":"markdown","source":["###### **Analyzing Transactions with Errors (Fraud Detection):**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3d39254d-2cd8-4dd2-8754-bc0c8faca551"},{"cell_type":"code","source":["# Count transactions with errors (fraud)\n","fraudulent_transactions = transactionsdf.filter(col(\"errors\").isNotNull()).count()\n","total_transactions = transactionsdf.count()\n","fraud_percentage = (fraudulent_transactions / total_transactions) * 100\n","print(f\"Percentage of fraudulent transactions: {fraud_percentage:.2f}%\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:13.799458Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"2ec27fa7-2f60-47eb-af32-2ba0c939136a"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Percentage of fraudulent transactions: 1.59%\n"]}],"execution_count":20,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b6145f6d-fe16-4967-aef1-4959572d0a32"},{"cell_type":"markdown","source":["###### **Fraudulent Transactions by Merchant ID** : This below analysis helps identify merchants involved in fraudulent transactions by counting them and summing the amounts. It’s useful for spotting high-risk merchants and focusing fraud prevention efforts where needed."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9ffc6b65-478f-4e2e-9bec-8e61c6fe98be"},{"cell_type":"code","source":["from pyspark.sql.functions import col, sum, count\n","# Fraudulent transactions by Merchant ID\n","fraud_by_merchant_id = transactionsdf.filter(col(\"errors\").isNotNull()).groupBy(\"client_id\") \\\n","    .agg(count(\"*\").alias(\"TransactionCount\"), sum(\"amount\").alias(\"TransactionSum\") \n","    ).withColumnRenamed(\"client_id\", \"Client\").orderBy(\"TransactionCount\", ascending=False)\n","fraud_by_merchant_id.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:14.0153296Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"70df7b6a-5668-4897-a72f-428f3e8ba309"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+------+----------------+------------------+\n|Client|TransactionCount|    TransactionSum|\n+------+----------------+------------------+\n|   954|            2935|         226724.76|\n|  1888|            2109| 90075.82999999999|\n|   464|            1196|         127925.28|\n|  1098|            1125|          42302.82|\n|  1696|             953|          59278.27|\n|   425|             847|          55073.41|\n|  1424|             822|          12968.02|\n|  1382|             821|          23333.85|\n|   373|             816|19862.280000000006|\n|   114|             796|          29223.93|\n|   476|             741|31039.879999999997|\n|  1885|             713|49210.490000000005|\n|    53|             675|          26174.66|\n|  1963|             654|19544.319999999996|\n|    96|             617|          66182.73|\n|  1340|             611| 74000.26999999999|\n|   208|             601|34501.700000000004|\n|  1797|             599|          31679.86|\n|   154|             593|45622.990000000005|\n|  1241|             552|48040.159999999996|\n+------+----------------+------------------+\nonly showing top 20 rows\n\n"]}],"execution_count":21,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6c9b341c-9328-4b10-b0ec-28e0abcd8bac"},{"cell_type":"markdown","source":["###### **Fraudulent Transactions by Zip Code** : This analysis tracks fraud by identifying suspicious transactions in different regions and over time. By grouping data based on zip code and year, it helps detect patterns and regional trends in fraudulent activities. This insight is crucial for businesses to target areas with higher fraud risk and enhance security measures accordingly."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7c4c89a5-6476-4a72-9c60-0c728a10d52a"},{"cell_type":"code","source":["# Fraudulent transactions by Zip Code\n","fraud_by_zipcode = transactionsdf.filter(col(\"errors\").isNotNull()).withColumn(\"Year\",date_format(\"date\",\"yyyy\")).groupBy(\"zip\", \"Year\") \\\n","    .agg(\n","        count(\"*\").alias(\"TransactionCount\"), sum(\"amount\").alias(\"TransactionSum\")\n","        ).withColumnRenamed('zip','ZipCode').select(\"Year\", \"TransactionCount\", \"TransactionSum\", \"ZipCode\").orderBy(\"Year\", ascending=False)\n","fraud_by_zipcode.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:14.1872603Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"6c61bf66-8071-4ff5-8bc7-ec23eb057ab9"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+----+----------------+------------------+-------+\n|Year|TransactionCount|    TransactionSum|ZipCode|\n+----+----------------+------------------+-------+\n|2019|              16|            735.48|71019.0|\n|2019|               1|             59.06|58425.0|\n|2019|               2|            859.94|11219.0|\n|2019|              13|            532.12|77086.0|\n|2019|               2|              24.5|85716.0|\n|2019|               5|            145.22|43812.0|\n|2019|               1|             45.62| 7646.0|\n|2019|               7|            405.36|53149.0|\n|2019|               5|            359.95|13031.0|\n|2019|               1|              67.0|54801.0|\n|2019|              31|           2423.49|39452.0|\n|2019|               2|102.46000000000001|23103.0|\n|2019|               2|            -81.29|62565.0|\n|2019|               2|49.239999999999995|95821.0|\n|2019|              15|            188.13|38606.0|\n|2019|              11|            377.53|92505.0|\n|2019|               2|140.42000000000002|56031.0|\n|2019|               4|            236.65|93308.0|\n|2019|               5|            131.54|11743.0|\n|2019|               1|             15.64|61073.0|\n+----+----------------+------------------+-------+\nonly showing top 20 rows\n\n"]}],"execution_count":22,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b4f9a102-e8c2-46f5-9229-e1feff854845"},{"cell_type":"markdown","source":["### **Client Data Analysis**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a29c4af3-3e47-48d8-bacf-b215b1d62852"},{"cell_type":"markdown","source":["###### **Client Demographics Analysis (Age, Gender, Income)**: This analysis provides a comprehensive view of the client base by examining key demographics like age, gender, and income. The `describe()` function is used to summarize the distribution of client age and income, while the `groupBy` operation helps to analyze gender distribution. Understanding these demographics helps businesses tailor their products, services, and marketing efforts to specific client segments, improving customer engagement and decision-making."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"312ad714-51ec-48b3-8242-1b18f513b486"},{"cell_type":"code","source":["# Age distribution of clients\n","clientdf.select(\"current_age\").describe().show()\n","\n","# Income distribution (per capita and yearly)\n","clientdf.select(\"per_capita_income\", \"yearly_income\").describe().show()\n","\n","# Gender distribution\n","clientdf.groupBy(\"gender\").count().show()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":null,"statement_ids":null,"state":"waiting","livy_statement_state":null,"session_id":null,"normalized_state":"waiting","queued_time":"2024-11-10T18:27:14.3862909Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"parent_msg_id":"6af1a0df-2d94-4939-afdf-d6af598b294d"},"text/plain":"StatementMeta(, , , Waiting, , Waiting)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-------+------------------+\n|summary|       current_age|\n+-------+------------------+\n|  count|              2000|\n|   mean|           45.3915|\n| stddev|18.414091537014993|\n|    min|                18|\n|    max|               101|\n+-------+------------------+\n\n+-------+-----------------+-----------------+\n|summary|per_capita_income|    yearly_income|\n+-------+-----------------+-----------------+\n|  count|             2000|             2000|\n|   mean|        23141.928|        45715.882|\n| stddev|11324.13735766499|22992.61545631198|\n|    min|              0.0|              1.0|\n|    max|         163145.0|         307018.0|\n+-------+-----------------+-----------------+\n\n+------+-----+\n|gender|count|\n+------+-----+\n|Female| 1016|\n|  Male|  984|\n+------+-----+\n\n"]}],"execution_count":23,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"18f069f6-9cb3-4623-9f85-54433d6223e3"},{"cell_type":"markdown","source":["**Average Credit Score**: It computes the average credit score of clients using the credit_score column, helping to understand the overall creditworthiness of the customer base.\n","\n","**Debt-to-Income Ratio**: It calculates the debt-to-income ratio by dividing total_debt by yearly_income, which is an important metric to assess financial health and risk.\n","\n","These calculations are useful in identifying trends in client financial health and assessing potential risk factors for loans or other financial services."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"12578ba2-2f97-4ed6-a422-5ae1a59ef49b"},{"cell_type":"code","source":["from pyspark.sql.functions import avg\n","# Calculate and print average credit score\n","avg_credit_score = clientdf.agg({\"credit_score\": \"avg\"}).collect()[0][0]\n","print(f\"The average credit score of clients is: {avg_credit_score}\")\n","#Making a Joined DataFrame with transactionsdf \n","CreditScoredf = transactionsdf.join(clientdf, transactionsdf.client_id == clientdf.id, \"inner\")\n","# calculating the average credit score group by ZipCode,\n","CreditScoredf = CreditScoredf.filter(col(\"zip\").isNotNull()).groupBy(\"zip\").agg(avg(\"credit_score\").alias(\"AverageCreditScore\")).orderBy(\"zip\")\n","CreditScoredf.show()\n","# Debt-to-Income ratio\n","clientdf.withColumn(\"debt_to_income\", col(\"total_debt\") / col(\"yearly_income\")).select(\"debt_to_income\").describe().show()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":33,"statement_ids":[33],"state":"finished","livy_statement_state":"available","session_id":"a32129fa-5b8b-427f-a9b8-0d65d191327e","normalized_state":"finished","queued_time":"2024-11-10T18:51:49.4206151Z","session_start_time":null,"execution_start_time":"2024-11-10T18:51:49.813526Z","execution_finish_time":"2024-11-10T18:51:56.2982004Z","parent_msg_id":"f26504aa-06fe-4a50-8df9-c717932c4a3e"},"text/plain":"StatementMeta(, a32129fa-5b8b-427f-a9b8-0d65d191327e, 33, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The average credit score of clients is: 709.7345\n+------+------------------+\n|   zip|AverageCreditScore|\n+------+------------------+\n|1001.0| 707.5777777777778|\n|1002.0| 681.4444444444445|\n|1007.0|             681.0|\n|1008.0|             655.0|\n|1009.0|             749.0|\n|1010.0|             683.0|\n|1011.0|             689.5|\n|1012.0| 697.2413793103449|\n|1013.0| 730.2083333333334|\n|1020.0|             682.0|\n|1022.0|             772.0|\n|1027.0|             748.6|\n|1028.0| 703.9532710280374|\n|1030.0|           700.125|\n|1031.0|             503.0|\n|1032.0| 691.3333333333334|\n|1033.0|           688.875|\n|1034.0| 733.6666666666666|\n|1035.0|             741.2|\n|1036.0| 720.3888888888889|\n+------+------------------+\nonly showing top 20 rows\n\n+-------+------------------+\n|summary|    debt_to_income|\n+-------+------------------+\n|  count|              2000|\n|   mean|1.3816930657665631|\n| stddev|0.8692247848503719|\n|    min|               0.0|\n|    max|  4.97864889017704|\n+-------+------------------+\n\n"]}],"execution_count":31,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"396e2e19-3a95-44aa-a809-c6289c51c81f"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}